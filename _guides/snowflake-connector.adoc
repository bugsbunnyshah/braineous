////
This guide is maintained in the main Quarkus repository
and pull requests should be submitted there:
https://github.com/quarkusio/quarkus/tree/main/docs/src/main/asciidoc
////
= Integrate the Snowflake Data Pipeline Connector
include::_attributes.adoc[]
:categories: snow-flake-connector
:summary: Discover how to integrate the Snowflake Data Pipeline Connector.
:numbered:
:sectnums:
:sectnumlevels: 4
:topics: snow-flake-connector

:url: https://www.dropbox.com/scl/fi/87ixa3bbegsmhaiz3danr/braineous-1.0.0-cr3.zip?rlkey=o4s7a8nvuwh8v1wuat1gl6uxe&st=9i9z0u2h&dl=0
:getting_started_url: https://bugsbunnyshah.github.io/braineous/get-started/
:imagesdir: ../assets

Learn how to integrate the Snowflake Data Pipeline Connector.

This guide covers:

* Get the Source Data
* Register a data pipe and send source data to configured target Snowflake database
* Verify the target database received the data

== Prerequisites

:prerequisites-no-graalvm:
include::{includes}/prerequisites.adoc[]

[TIP]
.Verify Maven is using the Java you expect
====
If you have multiple JDK's installed, it is not certain Maven will pick up the expected java
and you could end up with unexpected results.
You can verify which JDK Maven uses by running `mvn --version`.
====

Download the {url}[Braineous-1.0.0-CR3] zip archive

This tutorial is located under: braineous-1.0.0-cr3/tutorials/snowflake-connector

== Initialize
Get an instance of the `DataPlatformService`. Setup your `API_KEY` and `API_SECRET`.
Please refer to 'Step 9' of the Getting Started guide.
{getting_started_url}

[source,bash,subs=attributes+]
----
DataPlatformService dataPlatformService = DataPlatformService.getInstance();

String apiKey = "ffb2969c-5182-454f-9a0b-f3f2fb0ebf75";
String apiSecret = "5960253b-6645-41bf-b520-eede5754196e";
----

== Get the Source Data

Let's start with a simple Json array to be used as datasource to be
ingested by the *Braineous Data Ingestion Engine*

Source Data
[source,bash,subs=attributes+]
----
[
  {
    "tutorial" : "snowflake"
  }
]
----

Java Code
[source,bash,subs=attributes+]
----
String datasetLocation = "dataset/data.json";
String json = Util.loadResource(datasetLocation);
----
A dataset can be loaded from any data source such as a database, legacy production data store,
live data feed, third-party data source, Kafka stream, etc. In this example the dataset is loaded from a classpath
resource located at `src/main/resources/dataset/data.json`

== Register a data pipe and send source data to configured target Snowflake database

Register a data pipe with the *Braineous Data Ingestion Engine* using the
*Java Braineous Data Ingestion Client SDK*.

Pipe Configuration
[source,bash,subs=attributes+]
----
{
  "pipeId": "tutorial",
  "entity": "tutorial",
  "configuration": [
    {
      "stagingStore" : "com.appgallabs.dataplatform.targetSystem.core.driver.SnowflakeStagingStore",
      "name": "snowflake_staging_store",
      "config": {
        "account_identifier": "xxlpraf-ubb29207",
        "host": "xxlpraf-ubb29207.snowflakecomputing.com",
        "username": "******",
        "password": "*****",
        "port": "443",
        "database": "braineous",
        "schema": "public",
        "stage": "tutorial",
        "table": "tutorial",
        "pipe": "yyya",
        "source": "local_fs",
        "source_location":"file:///tmp/braineous/",
        "jsonpathExpressions": []
      }
    }
  ]
}
----

* *pipeId* : As a data source provider, this id identifies this data pipe
uniquely with the *Braineous Data Pipline Engine*.
* *entity* : The business/domain entity that this dataset should be associated with.
* *configuration.stagingStore*: The `Staging Store` driver
* *configuration.name*: a user-friendly way to indentify the target store
* *configuration.config.account_identifier*: Your Snowflake instance's account identifier
* *configuration.config.username*: Snowflake username
* *configuration.config.password*: Snowflake user's password
* *configuration.config.port*: Snowflake port
* *configuration.config.database*: Snowflake database
* *configuration.config.schema*: Snowflake database schema
* *configuration.config.stage*: Snowflake database staging area
* *configuration.config.table*: Snowflake database staging table
* *configuration.config.pipe*: Snowflake staging pipe that stages the data
* *configuration.config.source*: Currently Braineous supports local file system. Support for Amazon S3, Google Cloud Storage, and Miccrosoft Azure will be added in a future release
* *configuration.config.jsonpathExpressions*: Data Transformation based on JSONPath specification: https://www.ietf.org/archive/id/draft-goessner-dispatch-jsonpath-00.html

A data pipe can be configured with multiple target stores/systems associated with the same data pipe
for data delivery.

Java Code - Register Pipe
[source,bash,subs=attributes+]
----
String configLocation = "pipe_config/pipe_config.json";
String pipeConfigJson = Util.loadResource(configLocation);
JsonObject configJson = JsonUtil.validateJson(pipeConfigJson).getAsJsonObject();
String pipeId = configJson.get("pipeId").getAsString();
String entity = configJson.get("entity").getAsString();
System.out.println("*****PIPE_CONFIGURATION******");
JsonUtil.printStdOut(configJson);

//configure the DataPipeline Client
Configuration configuration = new Configuration().
ingestionHostUrl("http://localhost:8080/").
apiKey(apiKey).
apiSecret(apiSecret).
streamSizeInObjects(0);
dataPlatformService.configure(configuration);

//register pipe
dataPlatformService.registerPipe(configJson);
System.out.println("*****PIPE_REGISTRATION_SUCCESS******");
----

Pipe Configuration can be provided dynamically at runtime. The source can be a
database, a configuration system, local file system, network file system etc.
In this example the dataset is loaded from a classpath
resource located at `src/main/resources/pipe_config/pipe_config.json`


Java Code - Send Data for ingestion
[source,bash,subs=attributes+]
----
//send source data through the pipeline
dataPlatformService.sendData(pipeId, entity,datasetElement.toString());
System.out.println("*****DATA_INGESTION_SUCCESS******");
----

== Run the Tutorial

[source,bash,subs=attributes+]
----
cd braineous-1.0.0-cr3/tutorials/clickhouse-connector
----

[source,bash,subs=attributes+]
----
./run.sh
----

`Expected Output`:

[source,bash,subs=attributes+]
----
SLF4J: Failed to load class "org.slf4j.impl.StaticLoggerBinder".
SLF4J: Defaulting to no-operation (NOP) logger implementation
SLF4J: See http://www.slf4j.org/codes.html#StaticLoggerBinder for further details.
*****DATA_SET******
******ARRAY_SIZE: 1**********
[
  {
    "tutorial": "snowflake"
  }
]
**********************
*****PIPE_CONFIGURATION******
{
  "pipeId": "tutorial",
  "entity": "tutorial",
  "configuration": [
    {
      "stagingStore": "com.appgallabs.dataplatform.targetSystem.core.driver.SnowflakeStagingStore",
      "name": "snowflake_staging_store",
      "config": {
        "account_identifier": "xxlpraf-ubb29207",
        "host": "xxlpraf-ubb29207.snowflakecomputing.com",
        "username": "bugsbunnyshah",
        "password": "gagumaani@A61$",
        "port": "443",
        "database": "braineous",
        "schema": "public",
        "stage": "tutorial",
        "table": "tutorial",
        "pipe": "yyya",
        "source": "local_fs",
        "source_location": "file:///tmp/braineous/",
        "jsonpathExpressions": []
      }
    }
  ]
}
**********************
*****PIPE_REGISTRATION_SUCCESS******
***SENDING_DATA_START*****
*****DATA_INGESTION_SUCCESS******
----

== Verify all the data is successfully stored in the Snowflake Database

To verify the success of the ingestion and delivery to the configured target
databases, use the following login to your Snowflake instance

`Expected Result :` You should see the following results from the attached screenshots

image::tutorials/snowflake_pipe.png[]

image::tutorials/snowflake_table.png[]




